{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "02 Build_Hypothesis_cost.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMkB/nriMpCs0L39sNcKoPu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JinyongShin/TensorFlow/blob/main/02_Build_Hypothesis_cost.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "AFJ5-dW2djS0",
        "outputId": "9b08f735-1bcf-4836-a6f5-190d283d7742"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "tf.__version__"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'2.4.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acR58aAkdYTp"
      },
      "source": [
        "H(x) = Wx + b"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCSm9DlGdmYt"
      },
      "source": [
        "x_data = [1,2,3,4,5]\r\n",
        "y_data = [1,2,3,4,5]\r\n",
        "\r\n",
        "W = tf.Variable(2.9)\r\n",
        "b = tf.Variable(0.5)\r\n",
        "\r\n",
        "#Hypothesis = W*x + b\r\n",
        "hypothesis = W * x_data + b"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsI1sU-9eGwq"
      },
      "source": [
        "cost(W,b) = $\\frac{1}{m}$$\\sum_{i=1}^{m}$${(H(x^{(i)})-y^{(i)})}^{2}$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uZRaBUHZfM4K"
      },
      "source": [
        "cost = tf.reduce_mean(tf.square(hypothesis - y_data))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMjx4ZlAf9Wb"
      },
      "source": [
        "tf.reduce_mean() -> rank를 줄이고 mean을 계산\r\n",
        "\r\n",
        "tf.square() -> 제곱 계산"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tB5sgjoXgHjn",
        "outputId": "4b3d3537-4982-427d-91c8-9b71e7e98f0f"
      },
      "source": [
        "v=[1.,2.,3.,4.]\r\n",
        "tf.print(tf.reduce_mean(v))\r\n",
        "\r\n",
        "tf.print(tf.square(3))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.5\n",
            "9\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2MSz5_AJgrDX"
      },
      "source": [
        "Cost 를 minimize 하는 알고리즘 중 가장 유명한 Gradient descent(경사 하강 알고리즘)\r\n",
        "\r\n",
        "minimize cost(W,b)\r\n",
        "\r\n",
        " Tensor Flow 에서 Gradient descent는 일반적으로 GradientTape 으로 구현한다. with 구문과 함께 쓰이며, with 구문 내부에 있는 변수들의 변화(정보)를 tape에 저장한다. 이후에 tape에 gradient를 호출해서 미분값을 구한다. gradient 함수는 함수에 대해서 변수들에 대한 개별 미분값을 튜플로 순서대로 반환한다. \r\n",
        "\r\n",
        " A.assign_sub(B)\r\n",
        "\r\n",
        "python의 A-=B 와 같은 역할!\r\n",
        "\r\n",
        "learningrate는 grad 값을 얼마나 반영 할 것인가를 결정."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RSt0gfQ5NV2",
        "outputId": "b6ed81e8-2b09-4ca8-aa84-a2f4face61a7"
      },
      "source": [
        "#Learning_rate initialize\r\n",
        "learning_rate = 0.01\r\n",
        "\r\n",
        "#Gradient decent\r\n",
        "with tf.GradientTape() as tape: #with 구문 내부 변수의 정보를 GradientTape의 tape에 저장\r\n",
        "  hypothesis = W * x_data + b\r\n",
        "  cost = tf.reduce_mean(tf.square(hypothesis - y_data))\r\n",
        "\r\n",
        "W_grad, b_grad = tape.gradient(cost, [W,b]) #gradient 함수로 cost 함수의 W,b 변수에 대한 개별 미분값을 반환\r\n",
        "\r\n",
        "W.assign_sub(learning_rate * W_grad)\r\n",
        "b.assign_sub(learning_rate * b_grad)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'UnreadVariable' shape=() dtype=float32, numpy=0.376>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLkfTl-u8Cc2"
      },
      "source": [
        "Gradient decent 알고리즘을 한번만 수행하는 것이 아니고, 여러번 반복 수행 하며 W,b를 업데이트 한다. (for문을 사용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJ543HL88Twn",
        "outputId": "b384b904-8b7c-4d37-d522-10ebf203b866"
      },
      "source": [
        "for i in range(100):\r\n",
        "  #Gradient decent\r\n",
        "  with tf.GradientTape() as tape: #with 구문 내부 변수의 정보를 GradientTape의 tape에 저장\r\n",
        "    hypothesis = W * x_data + b\r\n",
        "    cost = tf.reduce_mean(tf.square(hypothesis - y_data))\r\n",
        "    \r\n",
        "  W_grad, b_grad = tape.gradient(cost, [W,b]) #gradient 함수로 cost 함수의 W,b 변수에 대한 개별 미분값을 반환\r\n",
        "  W.assign_sub(learning_rate * W_grad)\r\n",
        "  b.assign_sub(learning_rate * b_grad)\r\n",
        "\r\n",
        "  if i % 10 == 0: #i값이 10의 배수가 될 때 마다 출력!\r\n",
        "    print(\"{:5}|{:10.4}|{:10.4}|{:10.6f}\".format(i,W.numpy(),b.numpy(),cost))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    0|       1.0|-5.443e-05|  0.000000\n",
            "   10|       1.0|-5.264e-05|  0.000000\n",
            "   20|       1.0|-5.092e-05|  0.000000\n",
            "   30|       1.0|-4.924e-05|  0.000000\n",
            "   40|       1.0|-4.763e-05|  0.000000\n",
            "   50|       1.0|-4.609e-05|  0.000000\n",
            "   60|       1.0|-4.459e-05|  0.000000\n",
            "   70|       1.0|-4.313e-05|  0.000000\n",
            "   80|       1.0|-4.174e-05|  0.000000\n",
            "   90|       1.0|-4.041e-05|  0.000000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ0pYyWr90FZ"
      },
      "source": [
        "여러번 수행할 수록 가까운 값에 다가감.\r\n",
        "반복 수행하면, W 는 1로, b는 0으로 점점 수렴해 가는 것을 확인할 수 있음."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z5NcB_JR-TA-",
        "outputId": "eb7a4eaf-d6bc-46f4-b15e-74c34fff6229"
      },
      "source": [
        "print(W * 5 + b)\r\n",
        "print(W * 2.5 + b)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(5.0000157, shape=(), dtype=float32)\n",
            "tf.Tensor(2.499988, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBTcVu30-aR3"
      },
      "source": [
        "실제로 값이 잘 나오는 것을 한 번 더 확인할 수 있었다."
      ]
    }
  ]
}