{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "06 Softmax_Regression.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNGGg7ejM8h599gjvuEcMrd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JinyongShin/TensorFlow/blob/main/06_Softmax_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h36vszhrFjWD",
        "outputId": "8fedbdd9-7f13-4095-ab31-416dfc43f57b"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "print(tf.__version__)\n",
        "tf.random.set_seed(777)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xt1s2tl3jBLT"
      },
      "source": [
        "x_data = [[1, 2, 1,1],\n",
        "          [2, 1, 3, 2],\n",
        "          [3, 1, 3, 4],\n",
        "          [4, 1, 5, 5],\n",
        "          [1, 7, 5, 5],\n",
        "          [1, 2, 5, 6],\n",
        "          [1, 6, 6, 6],\n",
        "          [1, 7, 7, 7]]\n",
        "\n",
        "y_data = [[0, 0, 1], \n",
        "          [0, 0, 1],\n",
        "          [0, 0, 1],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [0, 1, 0],\n",
        "          [1, 0, 0],\n",
        "          [1, 0, 0]]\n",
        "\n",
        "#convert into numpy and float format\n",
        "x_data = np.asarray(x_data, dtype=np.float32)\n",
        "y_data = np.asarray(y_data, dtype=np.float32)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdX8sFhZkCe4",
        "outputId": "835a450b-7f68-4123-deb4-ba233423f889"
      },
      "source": [
        "nb_classes = 3 # number of classes\n",
        "\n",
        "print(x_data.shape)\n",
        "print(y_data.shape)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8, 4)\n",
            "(8, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UcxpBQ5GkYPF",
        "outputId": "b11701a1-f66a-4ada-9fba-dcb43d4ace88"
      },
      "source": [
        "#Weight and bias setting\n",
        "W = tf.Variable(tf.random.normal((4, nb_classes)), name='weight')\n",
        "b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')\n",
        "\n",
        "variables = [W , b]\n",
        "print(W , b)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<tf.Variable 'weight:0' shape=(4, 3) dtype=float32, numpy=\n",
            "array([[ 0.69555974, -0.25555828,  0.04393371],\n",
            "       [-1.3831443 ,  0.9659563 , -1.1959723 ],\n",
            "       [-0.44151336, -0.42753163, -0.19258936],\n",
            "       [ 0.0466683 , -0.16183038,  0.61364186]], dtype=float32)> <tf.Variable 'bias:0' shape=(3,) dtype=float32, numpy=array([-0.5442238 ,  0.22780192,  0.704233  ], dtype=float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAdykkZTlICj",
        "outputId": "1ca58fae-3fa8-4ebd-c7cb-a6d2960c8251"
      },
      "source": [
        "\"\"\"\n",
        "tf.nn.softmax computes softmax activations\n",
        "softmax = exp(logits)/reduce_sum(expt(logits),dim)\n",
        "\"\"\"\n",
        "def hypothesis(X) :\n",
        "  return tf.nn.softmax(tf.matmul(X, W) + b)\n",
        "\n",
        "print(hypothesis(x_data))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[1.2120243e-02 9.1549844e-01 7.2381392e-02]\n",
            " [9.2492111e-02 2.1506955e-01 6.9243836e-01]\n",
            " [7.2892822e-02 4.3151308e-02 8.8395578e-01]\n",
            " [5.1223580e-02 9.7793126e-03 9.3899715e-01]\n",
            " [2.2841135e-07 9.9990880e-01 9.0914138e-05]\n",
            " [3.2851368e-03 9.2520788e-02 9.0419406e-01]\n",
            " [2.9006392e-06 9.9783212e-01 2.1650069e-03]\n",
            " [3.3683312e-07 9.9931419e-01 6.8550062e-04]], shape=(8, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WRWPF8awmsb7",
        "outputId": "0ad36b2a-9293-4128-e21e-4adbc059b806"
      },
      "source": [
        "#softmax test\n",
        "sample_db = [[8,2,1,4]]\n",
        "sample_db = np.asarray(sample_db, dtype=np.float32)\n",
        "\n",
        "print(hypothesis(sample_db))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.7174977  0.03722719 0.24527511]], shape=(1, 3), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oxOzO2odn0rO",
        "outputId": "dae06574-e5a2-416f-9836-0d8765648817"
      },
      "source": [
        "# Cost function : cross entropy\n",
        "def cost_fn(X , Y) :\n",
        "  logits = hypothesis(X)\n",
        "  cost = -tf.reduce_sum(Y * tf.math.log(logits), axis=1)\n",
        "  cost_mean = tf.reduce_mean(cost)\n",
        "  return cost_mean\n",
        "\n",
        "print(cost_fn(x_data , y_data))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(4.722356, shape=(), dtype=float32)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G39kcPCIpeGI",
        "outputId": "8ab5873c-bb4b-4c28-f41e-68c9907261e0"
      },
      "source": [
        "#Gradient Function\n",
        "def grad_fn(X , Y):\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = cost_fn(X , Y)\n",
        "    grads = tape.gradient(loss , variables)\n",
        "    return grads\n",
        "\n",
        "print(grad_fn(x_data , y_data))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<tf.Tensor: shape=(4, 3), dtype=float32, numpy=\n",
            "array([[-0.17200428, -0.1745269 ,  0.34653118],\n",
            "       [-1.59407   ,  1.5331991 ,  0.06087089],\n",
            "       [-1.527395  ,  0.64792466,  0.8794704 ],\n",
            "       [-1.5294344 ,  0.513     ,  1.0164343 ]], dtype=float32)>, <tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.22099784,  0.15913431,  0.06186352], dtype=float32)>]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQ-tjWJkqKP8",
        "outputId": "01c6264a-8543-47a7-b7ba-f91d953d4aa6"
      },
      "source": [
        "#Train\n",
        "def fit(X, Y, epochs=2000 , verbose = 100):\n",
        "  optimizer = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "  for i in range(epochs):\n",
        "        grads = grad_fn(X, Y)\n",
        "        optimizer.apply_gradients(zip(grads, variables))\n",
        "        if (i==0) | ((i+1)%verbose==0):\n",
        "            print('Loss at epoch %d: %f' %(i+1, cost_fn(X, Y).numpy()))\n",
        "            \n",
        "fit(x_data, y_data)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at epoch 1: 3.609033\n",
            "Loss at epoch 100: 0.679015\n",
            "Loss at epoch 200: 0.597252\n",
            "Loss at epoch 300: 0.540765\n",
            "Loss at epoch 400: 0.490731\n",
            "Loss at epoch 500: 0.443269\n",
            "Loss at epoch 600: 0.396927\n",
            "Loss at epoch 700: 0.351063\n",
            "Loss at epoch 800: 0.305701\n",
            "Loss at epoch 900: 0.263351\n",
            "Loss at epoch 1000: 0.238793\n",
            "Loss at epoch 1100: 0.226854\n",
            "Loss at epoch 1200: 0.216104\n",
            "Loss at epoch 1300: 0.206297\n",
            "Loss at epoch 1400: 0.197314\n",
            "Loss at epoch 1500: 0.189055\n",
            "Loss at epoch 1600: 0.181435\n",
            "Loss at epoch 1700: 0.174385\n",
            "Loss at epoch 1800: 0.167843\n",
            "Loss at epoch 1900: 0.161757\n",
            "Loss at epoch 2000: 0.156082\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ul4Svj85rFjQ",
        "outputId": "598c6b6b-364b-4cb7-c99b-5ba035caaec9"
      },
      "source": [
        "#Prediction Check\n",
        "\n",
        "sample_data = [[2, 1, 3, 2]] # answer label [[0,0,1]]\n",
        "sample_data = np.asarray(sample_data, dtype=np.float32)\n",
        "\n",
        "a = hypothesis(sample_data)\n",
        "\n",
        "print(a)\n",
        "print(tf.argmax(a,1))  # index : 2"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[0.00157137 0.08036441 0.91806424]], shape=(1, 3), dtype=float32)\n",
            "tf.Tensor([2], shape=(1,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JT5_al3Or0OU",
        "outputId": "51483c5f-0fab-484f-c688-5da2712b37d2"
      },
      "source": [
        "b = hypothesis(x_data)\n",
        "print(b)\n",
        "print(tf.argmax(b,1))\n",
        "print(tf.argmax(y_data,1))  #matches with y_data"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[[2.83591316e-06 1.30188034e-03 9.98695314e-01]\n",
            " [1.57136726e-03 8.03644136e-02 9.18064237e-01]\n",
            " [1.44851782e-07 1.61430359e-01 8.38569462e-01]\n",
            " [4.64985897e-06 8.52740943e-01 1.47254378e-01]\n",
            " [2.56197989e-01 7.31875956e-01 1.19259795e-02]\n",
            " [1.34828150e-01 8.65119934e-01 5.18369197e-05]\n",
            " [7.49832153e-01 2.50122398e-01 4.54276415e-05]\n",
            " [9.21672821e-01 7.83264562e-02 7.90108118e-07]], shape=(8, 3), dtype=float32)\n",
            "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n",
            "tf.Tensor([2 2 2 1 1 1 0 0], shape=(8,), dtype=int64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9jLvyl9sMc_"
      },
      "source": [
        "Convert as Class"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFa3AIpjsEF7",
        "outputId": "f0a4435b-5ec6-47ee-a6d8-b58323db1819"
      },
      "source": [
        "class softmax_classifer(tf.keras.Model):\n",
        "    def __init__(self, nb_classes):\n",
        "        super(softmax_classifer, self).__init__()\n",
        "        self.W = tf.Variable(tf.random.normal((4, nb_classes)), name='weight')\n",
        "        self.b = tf.Variable(tf.random.normal((nb_classes,)), name='bias')\n",
        "        \n",
        "    def softmax_regression(self, X):\n",
        "        return tf.nn.softmax(tf.matmul(X, self.W) + self.b)\n",
        "    \n",
        "    def cost_fn(self, X, Y):\n",
        "        logits = self.softmax_regression(X)\n",
        "        cost = tf.reduce_mean(-tf.reduce_sum(Y * tf.math.log(logits), axis=1))        \n",
        "        return cost\n",
        "    \n",
        "    def grad_fn(self, X, Y):\n",
        "        with tf.GradientTape() as tape:\n",
        "            cost = self.cost_fn(x_data, y_data)\n",
        "            grads = tape.gradient(cost, self.variables)            \n",
        "            return grads\n",
        "    \n",
        "    def fit(self, X, Y, epochs=2000, verbose=500):\n",
        "        optimizer =  tf.keras.optimizers.SGD(learning_rate=0.1)\n",
        "\n",
        "        for i in range(epochs):\n",
        "            grads = self.grad_fn(X, Y)\n",
        "            optimizer.apply_gradients(zip(grads, self.variables))\n",
        "            if (i==0) | ((i+1)%verbose==0):\n",
        "                print('Loss at epoch %d: %f' %(i+1, self.cost_fn(X, Y).numpy()))\n",
        "            \n",
        "model = softmax_classifer(nb_classes)\n",
        "model.fit(x_data, y_data)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loss at epoch 1: 2.347447\n",
            "Loss at epoch 500: 0.448661\n",
            "Loss at epoch 1000: 0.273383\n",
            "Loss at epoch 1500: 0.198863\n",
            "Loss at epoch 2000: 0.163236\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}